{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project tasks\n",
    "\n",
    "- Please rename this file so that you know which copy you have been working in. Keep a copy safe (especially if you are working in the online Jupyter service). You can download a copy by choosing -File- then -Download as- Notebook from the menu above. \n",
    "- Complete all of the tasks \n",
    "- Make sure to use good code style, and comment your code appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Code review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task is to write a code review, *not* to write python code to solve the problem brief. \n",
    "\n",
    "A colleague has been asked to write some code to answer the following brief:\n",
    "\n",
    "---\n",
    "\n",
    "### Brief:\n",
    "The determinant of an $n\\times n$ matrix $A$ can be calculated recursively by \"row/column expansion\", for any given column $j$:\n",
    "\n",
    "$$\n",
    "\\textrm{det}(A) = \\sum_{i=1}^n (−1)^{(i+j)}\\,\\,a_{ij}\\,\\, \\textrm{det}(\\bar{A}_{ij})\n",
    "$$\n",
    "\n",
    "where $a_{ij}$ is the element of $A$ in the $i$-th row and $j$-th column, and $\\bar{A}_{ij}$ is the $(n − 1)\\times (n − 1)$ matrix obtained from $A$ by deleting the $i$-th row and $j$-th column.\n",
    "\n",
    "The above formula works for any $j$ (you can just use $j = 1$: expansion of the first column).\n",
    "\n",
    "You should:\n",
    "\n",
    "* Write a function to (recursively) work out the determinant of a two-dimensional Numpy array without using the inbuilt functions. \n",
    "\n",
    "* Write a further function which determines the volume of the parallelepiped spanned by the vectors $(x_1,y_1,z_1)$, $(x_2,y_2,z_2)$, $(x_3,y_3,z_3)$, which is given by the absolute value of:\n",
    "\n",
    "$$\n",
    "\\textrm{det}\\left(\\begin{array}{ccc}\n",
    "x_1 & y_1 & z_1\\\\\n",
    "x_2 & y_2 & z_2\\\\\n",
    "x_3 & y_3 & z_3\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "using your determinant function.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Your task:\n",
    "\n",
    "You have been asked to write a review of their code. Here is the code they produced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from numpy import *\n",
    "\n",
    "def my_det(A,n):\n",
    "# Calculate the determinant of a matrix\n",
    "    if n==1:\n",
    "        return A[0,0]  #Return a single value in this case\n",
    "    else:\n",
    "        det = 0.0\n",
    "    for i in range(0,n-1):\n",
    "          # Remove the row and column:  \n",
    "            B = delete(A,i,axis=0)\n",
    "            C = delete(B,0,axis=1)\n",
    "          #print(C)\n",
    "            det = det + (-1)**(i+1)*A[i][0]*my_det(C,n-1) #implement the formula\n",
    "    return det\n",
    "\n",
    "def paraVol(a,b,c):\n",
    "  '''\n",
    "  Get the volume of a parallelepiped spanned by a,b,c.\n",
    "  '''\n",
    "  A = array([[a[0],a[1],a[2]],[b[0],b[1],b[2]],[c[0],c[1],c[2]]])\n",
    "  return my_det(A,3)\n",
    "\n",
    " # Test on the identity matrix:\n",
    "A = identity(3)\n",
    "print(my_det(A,3))\n",
    "print(linalg.det(A))  #compare with built-in function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should write your review here. \n",
    "Things you could choose to discuss:\n",
    "- Code structure \n",
    "- Code style \n",
    "- Does it answer the brief?\n",
    "- Does it work? If not could it be fixed?\n",
    "- Can you explain what it does?\n",
    "\n",
    "Keep your answer relatively brief (approx. 500 words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - K-Nearest-neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2a - First implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.52101, 13.64, 4.49, 1.1, 71.78, 0.06, 8.75], [1.51761, 13.89, 3.6, 1.36, 72.73, 0.48, 7.83], [1.51618, 13.53, 3.55, 1.54, 72.99, 0.39, 7.78], [1.51766, 13.21, 3.69, 1.29, 72.61, 0.57, 8.22], [1.51742, 13.27, 3.62, 1.24, 73.08, 0.55, 8.07], [1.51596, 12.79, 3.61, 1.62, 72.97, 0.64, 8.07], [1.51743, 13.3, 3.6, 1.14, 73.09, 0.58, 8.17], [1.51574, 14.86, 3.67, 1.74, 71.87, 0.16, 7.36], [1.51848, 13.64, 3.87, 1.27, 71.96, 0.54, 8.32], [1.51593, 13.09, 3.59, 1.52, 73.1, 0.67, 7.83], [1.51631, 13.34, 3.57, 1.57, 72.87, 0.61, 7.89], [1.51596, 13.02, 3.56, 1.54, 73.11, 0.72, 7.9], [1.5159, 13.02, 3.58, 1.51, 73.12, 0.69, 7.96], [1.51645, 13.44, 3.61, 1.54, 72.39, 0.66, 8.03], [1.51769, 13.65, 3.66, 1.11, 72.77, 0.11, 8.6], [1.5161, 13.33, 3.53, 1.34, 72.67, 0.56, 8.33], [1.5167, 13.24, 3.57, 1.38, 72.7, 0.56, 8.44], [1.51514, 14.01, 2.68, 3.5, 69.89, 1.68, 5.87], [1.51915, 12.73, 1.85, 1.86, 72.69, 0.6, 10.09], [1.52171, 11.56, 1.88, 1.56, 72.86, 0.47, 11.41], [1.51905, 14.0, 2.39, 1.56, 72.37, 0.0, 9.57], [1.51937, 13.79, 2.41, 1.19, 72.76, 0.0, 9.77], [1.51829, 14.46, 2.24, 1.62, 72.38, 0.0, 9.26], [1.51131, 13.69, 3.2, 1.81, 72.81, 1.76, 5.43], [1.51838, 14.32, 3.26, 2.22, 71.25, 1.46, 5.79], [1.52315, 13.44, 3.34, 1.23, 72.38, 0.6, 8.83]] [[1.51756, 13.15, 3.61, 1.05, 73.24, 0.57, 8.24, 1.0], [1.51918, 14.04, 3.58, 1.37, 72.08, 0.56, 8.3, 1.0], [1.51755, 13.0, 3.6, 1.36, 72.99, 0.57, 8.4, 1.0], [1.51571, 12.72, 3.46, 1.56, 73.2, 0.67, 8.09, 1.0], [1.51763, 12.8, 3.66, 1.27, 73.01, 0.6, 8.56, 1.0], [1.51589, 12.88, 3.43, 1.4, 73.28, 0.69, 8.05, 1.0], [1.51748, 12.86, 3.56, 1.27, 73.21, 0.54, 8.38, 1.0], [1.51763, 12.61, 3.59, 1.31, 73.29, 0.58, 8.5, 1.0], [1.51761, 12.81, 3.54, 1.23, 73.24, 0.58, 8.39, 1.0], [1.51784, 12.68, 3.67, 1.16, 73.11, 0.61, 8.7, 1.0], [1.52196, 14.36, 3.85, 0.89, 71.36, 0.15, 9.15, 1.0], [1.51911, 13.9, 3.73, 1.18, 72.12, 0.06, 8.89, 1.0], [1.51735, 13.02, 3.54, 1.69, 72.73, 0.54, 8.44, 1.0], [1.5175, 12.82, 3.55, 1.49, 72.75, 0.54, 8.52, 1.0], [1.51966, 14.77, 3.75, 0.29, 72.02, 0.03, 9.0, 1.0], [1.51736, 12.78, 3.62, 1.29, 72.79, 0.59, 8.7, 1.0], [1.51751, 12.81, 3.57, 1.35, 73.02, 0.62, 8.59, 1.0], [1.5172, 13.38, 3.5, 1.15, 72.85, 0.5, 8.43, 1.0], [1.51764, 12.98, 3.54, 1.21, 73.0, 0.65, 8.53, 1.0], [1.51793, 13.21, 3.48, 1.41, 72.64, 0.59, 8.43, 1.0], [1.51721, 12.87, 3.48, 1.33, 73.04, 0.56, 8.43, 1.0], [1.51768, 12.56, 3.52, 1.43, 73.15, 0.57, 8.54, 1.0], [1.51784, 13.08, 3.49, 1.28, 72.86, 0.6, 8.49, 1.0], [1.51768, 12.65, 3.56, 1.3, 73.08, 0.61, 8.69, 1.0], [1.51747, 12.84, 3.5, 1.14, 73.27, 0.56, 8.55, 1.0], [1.51775, 12.85, 3.48, 1.23, 72.97, 0.61, 8.56, 1.0], [1.51753, 12.57, 3.47, 1.38, 73.39, 0.6, 8.55, 1.0], [1.51783, 12.69, 3.54, 1.34, 72.95, 0.57, 8.75, 1.0], [1.51567, 13.29, 3.45, 1.21, 72.74, 0.56, 8.57, 1.0], [1.51909, 13.89, 3.53, 1.32, 71.81, 0.51, 8.78, 1.0], [1.51797, 12.74, 3.48, 1.35, 72.96, 0.64, 8.68, 1.0], [1.52213, 14.21, 3.82, 0.47, 71.77, 0.11, 9.57, 1.0], [1.52213, 14.21, 3.82, 0.47, 71.77, 0.11, 9.57, 1.0], [1.51793, 12.79, 3.5, 1.12, 73.03, 0.64, 8.77, 1.0], [1.51755, 12.71, 3.42, 1.2, 73.2, 0.59, 8.64, 1.0], [1.51779, 13.21, 3.39, 1.33, 72.76, 0.59, 8.59, 1.0], [1.5221, 13.73, 3.84, 0.72, 71.76, 0.17, 9.74, 1.0], [1.51786, 12.73, 3.43, 1.19, 72.95, 0.62, 8.76, 1.0], [1.519, 13.49, 3.48, 1.35, 71.95, 0.55, 9.0, 1.0], [1.51869, 13.19, 3.37, 1.18, 72.72, 0.57, 8.83, 1.0], [1.52667, 13.99, 3.7, 0.71, 71.57, 0.02, 9.82, 1.0], [1.52223, 13.21, 3.77, 0.79, 71.99, 0.13, 10.02, 1.0], [1.51898, 13.58, 3.35, 1.23, 72.08, 0.59, 8.91, 1.0], [1.5232, 13.72, 3.72, 0.51, 71.75, 0.09, 10.06, 1.0], [1.51926, 13.2, 3.33, 1.28, 72.36, 0.6, 9.14, 1.0], [1.51808, 13.43, 2.87, 1.19, 72.84, 0.55, 9.03, 1.0], [1.51837, 13.14, 2.84, 1.28, 72.85, 0.55, 9.07, 1.0], [1.51778, 13.21, 2.81, 1.29, 72.98, 0.51, 9.02, 1.0], [1.51769, 12.45, 2.71, 1.29, 73.7, 0.56, 9.06, 1.0], [1.51215, 12.99, 3.47, 1.12, 72.98, 0.62, 8.35, 1.0], [1.51824, 12.87, 3.48, 1.29, 72.95, 0.6, 8.43, 1.0], [1.51754, 13.48, 3.74, 1.17, 72.99, 0.59, 8.03, 1.0], [1.51754, 13.39, 3.66, 1.19, 72.79, 0.57, 8.27, 1.0], [1.51905, 13.6, 3.62, 1.11, 72.64, 0.14, 8.76, 1.0], [1.51977, 13.81, 3.58, 1.32, 71.72, 0.12, 8.67, 1.0], [1.52172, 13.51, 3.86, 0.88, 71.79, 0.23, 9.54, 1.0], [1.52227, 14.17, 3.81, 0.78, 71.35, 0.0, 9.69, 1.0], [1.52172, 13.48, 3.74, 0.9, 72.01, 0.18, 9.61, 1.0], [1.52099, 13.69, 3.59, 1.12, 71.96, 0.09, 9.4, 1.0], [1.52152, 13.05, 3.65, 0.87, 72.22, 0.19, 9.85, 1.0], [1.52152, 13.05, 3.65, 0.87, 72.32, 0.19, 9.85, 1.0], [1.52152, 13.12, 3.58, 0.9, 72.2, 0.23, 9.82, 1.0], [1.523, 13.31, 3.58, 0.82, 71.99, 0.12, 10.17, 1.0], [1.51627, 13.0, 3.58, 1.54, 72.83, 0.61, 8.04, 2.0], [1.51613, 13.92, 3.52, 1.25, 72.88, 0.37, 7.94, 2.0], [1.5159, 12.82, 3.52, 1.9, 72.86, 0.69, 7.97, 2.0], [1.51592, 12.86, 3.52, 2.12, 72.66, 0.69, 7.97, 2.0], [1.51593, 13.25, 3.45, 1.43, 73.17, 0.61, 7.86, 2.0], [1.51646, 13.41, 3.55, 1.25, 72.81, 0.68, 8.1, 2.0], [1.51594, 13.09, 3.52, 1.55, 72.87, 0.68, 8.05, 2.0], [1.51409, 14.25, 3.09, 2.08, 72.28, 1.1, 7.08, 2.0], [1.51625, 13.36, 3.58, 1.49, 72.72, 0.45, 8.21, 2.0], [1.51569, 13.24, 3.49, 1.47, 73.25, 0.38, 8.03, 2.0], [1.51645, 13.4, 3.49, 1.52, 72.65, 0.67, 8.08, 2.0], [1.51618, 13.01, 3.5, 1.48, 72.89, 0.6, 8.12, 2.0], [1.5164, 12.55, 3.48, 1.87, 73.23, 0.63, 8.08, 2.0], [1.51841, 12.93, 3.74, 1.11, 72.28, 0.64, 8.96, 2.0], [1.51605, 12.9, 3.44, 1.45, 73.06, 0.44, 8.27, 2.0], [1.51588, 13.12, 3.41, 1.58, 73.26, 0.07, 8.39, 2.0], [1.5159, 13.24, 3.34, 1.47, 73.1, 0.39, 8.22, 2.0], [1.51629, 12.71, 3.33, 1.49, 73.28, 0.67, 8.24, 2.0], [1.5186, 13.36, 3.43, 1.43, 72.26, 0.51, 8.6, 2.0], [1.51841, 13.02, 3.62, 1.06, 72.34, 0.64, 9.13, 2.0], [1.51743, 12.2, 3.25, 1.16, 73.55, 0.62, 8.9, 2.0], [1.51689, 12.67, 2.88, 1.71, 73.21, 0.73, 8.54, 2.0], [1.51811, 12.96, 2.96, 1.43, 72.92, 0.6, 8.79, 2.0], [1.51655, 12.75, 2.85, 1.44, 73.27, 0.57, 8.79, 2.0], [1.5173, 12.35, 2.72, 1.63, 72.87, 0.7, 9.23, 2.0], [1.5182, 12.62, 2.76, 0.83, 73.81, 0.35, 9.42, 2.0], [1.52725, 13.8, 3.15, 0.66, 70.57, 0.08, 11.64, 2.0], [1.5241, 13.83, 2.9, 1.17, 71.15, 0.08, 10.79, 2.0], [1.52475, 11.45, 0.0, 1.88, 72.19, 0.81, 13.24, 2.0], [1.53125, 10.73, 0.0, 2.1, 69.81, 0.58, 13.3, 2.0], [1.53393, 12.3, 0.0, 1.0, 70.16, 0.12, 16.19, 2.0], [1.52222, 14.43, 0.0, 1.0, 72.67, 0.1, 11.52, 2.0], [1.51818, 13.72, 0.0, 0.56, 74.45, 0.0, 10.99, 2.0], [1.52664, 11.23, 0.0, 0.77, 73.21, 0.0, 14.68, 2.0], [1.52739, 11.02, 0.0, 0.75, 73.08, 0.0, 14.96, 2.0], [1.52777, 12.64, 0.0, 0.67, 72.02, 0.06, 14.4, 2.0], [1.51892, 13.46, 3.83, 1.26, 72.55, 0.57, 8.21, 2.0], [1.51847, 13.1, 3.97, 1.19, 72.44, 0.6, 8.43, 2.0], [1.51846, 13.41, 3.89, 1.33, 72.38, 0.51, 8.28, 2.0], [1.51829, 13.24, 3.9, 1.41, 72.33, 0.55, 8.31, 2.0], [1.51708, 13.72, 3.68, 1.81, 72.06, 0.64, 7.88, 2.0], [1.51673, 13.3, 3.64, 1.53, 72.53, 0.65, 8.03, 2.0], [1.51652, 13.56, 3.57, 1.47, 72.45, 0.64, 7.96, 2.0], [1.51844, 13.25, 3.76, 1.32, 72.4, 0.58, 8.42, 2.0], [1.51663, 12.93, 3.54, 1.62, 72.96, 0.64, 8.03, 2.0], [1.51687, 13.23, 3.54, 1.48, 72.84, 0.56, 8.1, 2.0], [1.51707, 13.48, 3.48, 1.71, 72.52, 0.62, 7.99, 2.0], [1.52177, 13.2, 3.68, 1.15, 72.75, 0.54, 8.52, 2.0], [1.51872, 12.93, 3.66, 1.56, 72.51, 0.58, 8.55, 2.0], [1.51667, 12.94, 3.61, 1.26, 72.75, 0.56, 8.6, 2.0], [1.52081, 13.78, 2.28, 1.43, 71.99, 0.49, 9.85, 2.0], [1.52068, 13.55, 2.09, 1.67, 72.18, 0.53, 9.57, 2.0], [1.5202, 13.98, 1.35, 1.63, 71.76, 0.39, 10.56, 2.0], [1.52177, 13.75, 1.01, 1.36, 72.19, 0.33, 11.14, 2.0], [1.52614, 13.7, 0.0, 1.36, 71.24, 0.19, 13.44, 2.0], [1.51813, 13.43, 3.98, 1.18, 72.49, 0.58, 8.15, 2.0], [1.518, 13.71, 3.93, 1.54, 71.81, 0.54, 8.21, 2.0], [1.51811, 13.33, 3.85, 1.25, 72.78, 0.52, 8.12, 2.0], [1.51789, 13.19, 3.9, 1.3, 72.33, 0.55, 8.44, 2.0], [1.51806, 13.0, 3.8, 1.08, 73.07, 0.56, 8.38, 2.0], [1.51711, 12.89, 3.62, 1.57, 72.96, 0.61, 8.11, 2.0], [1.51674, 12.79, 3.52, 1.54, 73.36, 0.66, 7.9, 2.0], [1.51674, 12.87, 3.56, 1.64, 73.14, 0.65, 7.99, 2.0], [1.5169, 13.33, 3.54, 1.61, 72.54, 0.68, 8.11, 2.0], [1.51851, 13.2, 3.63, 1.07, 72.83, 0.57, 8.41, 2.0], [1.51662, 12.85, 3.51, 1.44, 73.01, 0.68, 8.23, 2.0], [1.51709, 13.0, 3.47, 1.79, 72.72, 0.66, 8.18, 2.0], [1.5166, 12.99, 3.18, 1.23, 72.97, 0.58, 8.81, 2.0], [1.51839, 12.85, 3.67, 1.24, 72.57, 0.62, 8.68, 2.0], [1.51643, 12.16, 3.52, 1.35, 72.89, 0.57, 8.53, 3.0], [1.51665, 13.14, 3.45, 1.76, 72.48, 0.6, 8.38, 3.0], [1.52127, 14.32, 3.9, 0.83, 71.5, 0.0, 9.49, 3.0], [1.51779, 13.64, 3.65, 0.65, 73.0, 0.06, 8.93, 3.0], [1.5161, 13.42, 3.4, 1.22, 72.69, 0.59, 8.32, 3.0], [1.51694, 12.86, 3.58, 1.31, 72.61, 0.61, 8.79, 3.0], [1.51646, 13.04, 3.4, 1.26, 73.01, 0.52, 8.58, 3.0], [1.51655, 13.41, 3.39, 1.28, 72.64, 0.52, 8.65, 3.0], [1.52121, 14.03, 3.76, 0.58, 71.79, 0.11, 9.65, 3.0], [1.51776, 13.53, 3.41, 1.52, 72.04, 0.58, 8.79, 3.0], [1.51796, 13.5, 3.36, 1.63, 71.94, 0.57, 8.81, 3.0], [1.51832, 13.33, 3.34, 1.54, 72.14, 0.56, 8.99, 3.0], [1.51934, 13.64, 3.54, 0.75, 72.65, 0.16, 8.89, 3.0], [1.52211, 14.19, 3.78, 0.91, 71.36, 0.23, 9.14, 3.0], [1.52151, 11.03, 1.71, 1.56, 73.44, 0.58, 11.62, 5.0], [1.51969, 12.64, 0.0, 1.65, 73.75, 0.38, 11.53, 5.0], [1.51666, 12.86, 0.0, 1.83, 73.88, 0.97, 10.17, 5.0], [1.51994, 13.27, 0.0, 1.76, 73.03, 0.47, 11.32, 5.0], [1.52369, 13.44, 0.0, 1.58, 72.22, 0.32, 12.24, 5.0], [1.51316, 13.02, 0.0, 3.04, 70.48, 6.21, 6.96, 5.0], [1.51321, 13.0, 0.0, 3.02, 70.7, 6.21, 6.93, 5.0], [1.52043, 13.38, 0.0, 1.4, 72.25, 0.33, 12.5, 5.0], [1.52058, 12.85, 1.61, 2.17, 72.18, 0.76, 9.7, 5.0], [1.52119, 12.97, 0.33, 1.51, 73.39, 0.13, 11.27, 5.0], [1.51852, 14.09, 2.19, 1.66, 72.67, 0.0, 9.32, 6.0], [1.51299, 14.4, 1.74, 1.54, 74.55, 0.0, 7.59, 6.0], [1.51888, 14.99, 0.78, 1.74, 72.5, 0.0, 9.95, 6.0], [1.51916, 14.15, 0.0, 2.09, 72.74, 0.0, 10.88, 6.0], [1.51969, 14.56, 0.0, 0.56, 73.48, 0.0, 11.22, 6.0], [1.51115, 17.38, 0.0, 0.34, 75.41, 0.0, 6.65, 6.0], [1.52247, 14.86, 2.2, 2.06, 70.26, 0.76, 9.76, 7.0], [1.52365, 15.79, 1.83, 1.31, 70.43, 0.31, 8.61, 7.0], [1.51613, 13.88, 1.78, 1.79, 73.1, 0.0, 8.67, 7.0], [1.51602, 14.85, 0.0, 2.38, 73.28, 0.0, 8.76, 7.0], [1.51623, 14.2, 0.0, 2.79, 73.46, 0.04, 9.04, 7.0], [1.51719, 14.75, 0.0, 2.0, 73.02, 0.0, 8.53, 7.0], [1.51683, 14.56, 0.0, 1.98, 73.29, 0.0, 8.52, 7.0], [1.51545, 14.14, 0.0, 2.68, 73.39, 0.08, 9.07, 7.0], [1.51556, 13.87, 0.0, 2.54, 73.23, 0.14, 9.41, 7.0], [1.51727, 14.7, 0.0, 2.34, 73.28, 0.0, 8.95, 7.0], [1.51531, 14.38, 0.0, 2.66, 73.1, 0.04, 9.08, 7.0], [1.51609, 15.01, 0.0, 2.51, 73.05, 0.05, 8.83, 7.0], [1.51508, 15.15, 0.0, 2.25, 73.5, 0.0, 8.34, 7.0], [1.51653, 11.95, 0.0, 1.19, 75.18, 2.7, 8.93, 7.0], [1.51514, 14.85, 0.0, 2.42, 73.72, 0.0, 8.39, 7.0], [1.51658, 14.8, 0.0, 1.99, 73.11, 0.0, 8.28, 7.0], [1.51617, 14.95, 0.0, 2.27, 73.3, 0.0, 8.71, 7.0], [1.51732, 14.95, 0.0, 1.8, 72.99, 0.0, 8.61, 7.0], [1.51645, 14.94, 0.0, 1.87, 73.11, 0.0, 8.67, 7.0], [1.51831, 14.39, 0.0, 1.82, 72.86, 1.41, 6.47, 7.0], [1.5164, 14.37, 0.0, 2.74, 72.85, 0.0, 9.45, 7.0], [1.51623, 14.14, 0.0, 2.88, 72.61, 0.08, 9.18, 7.0], [1.51685, 14.92, 0.0, 1.99, 73.06, 0.0, 8.4, 7.0], [1.52065, 14.36, 0.0, 2.02, 73.42, 0.0, 8.44, 7.0], [1.51651, 14.38, 0.0, 1.94, 73.61, 0.0, 8.48, 7.0], [1.51711, 14.23, 0.0, 2.08, 73.36, 0.0, 8.62, 7.0]]\n",
      "[(1.999999999990898e-05, 1.0), (1.999999999990898e-05, 1.0), (1.999999999990898e-05, 1.0)]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statistics as st\n",
    "\n",
    "def data_setup():\n",
    "    main_data=[]\n",
    "    train_data=[]\n",
    "    with open('main_data.txt') as m:     #open file \n",
    "        for line in m:\n",
    "            m_inner_list = [float(elt.strip()) for elt in line.split(',')]  #convert each line to inner list \n",
    "            main_data.append(m_inner_list)   #find list of lists\n",
    "    with open('train_data.txt') as t:    #repeat some steps to train_data.txt\n",
    "        for line in t:\n",
    "            t_inner_list = [float(elt.strip()) for elt in line.split(',')]\n",
    "            train_data.append(t_inner_list)\n",
    "    return main_data,train_data\n",
    "\n",
    "def dist_vect(item,train_data):\n",
    "    a=[]                        #empty list to store tuples \n",
    "    for t_item in train_data:   #loop over train_data line by line\n",
    "        distance=0\n",
    "        for j in range(len(item)):  #sum distance in order\n",
    "            distance=distance+np.sqrt((item[j]-t_item[j])**2) #Euclidean distance\n",
    "            t=distance,t_item[7]    #class is the last element of each inner list in train_data \n",
    "            a.append(t)   #create list of tuples\n",
    "    e=sorted(a,key=lambda tup:tup[0])  #sorted by distance\n",
    "    dv=[e[0],e[1],e[2]]     #find the 3 nearest neighbours\n",
    "    return dv  \n",
    "\n",
    "def decide_class(dv):\n",
    "    votes=[x[1] for x in dv]  #creat a list of class \n",
    "    if votes[0]!=votes[1]!=votes[2]:  \n",
    "        item_class=votes[0] #chose the nearest one if there is no majority \n",
    "    else:\n",
    "        item_class=st.mode(votes) #find the most frequent class           \n",
    "    return item_class\n",
    "\n",
    "print(data_setup()[0],data_setup()[1])\n",
    "print(dist_vect(data_setup()[0][3],data_setup()[1]))\n",
    "print(decide_class(dist_vect(data_setup()[0][3],data_setup()[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task you will need to implement a [K-nearest-neighbours](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) algorithm for classification from scratch (**important - do not use a version of this algorithm from another module e.g SKLearn - you need to write the functions as directed yourself**). The K-nearest-neighbours algorithm is a classic machine learning algorithm used for [classification problems](https://en.wikipedia.org/wiki/Statistical_classification). Classifying a data item from a given dataset means deciding which of a number of classes the item belongs to. This is done using a *training set*, containing a number of data items with known class.\n",
    "\n",
    "We will consider first a simple implementation of the 3-nearest-neighbour version of the algorithm. The algorithm proceeds as follows.\n",
    "\n",
    "For each item in the dataset, we find the 3 nearest items (and their respective classes) in the training set. We then attribute a class to our data item by majority voting amongst the classes of the 3 nearest neighbours. In the case where three classes are tied in the vote, we resolve the tie by choosing the class of whichever neighbour is the nearest in the training set. (In practice this might not be a very good thing to do - but we are just constructing a simple implementation for now). Finally, we need to output the class we have decided on for each item in our original dataset.\n",
    "\n",
    "The data is contained in two files `main_data.txt` and `train_data.txt`.\n",
    "* Each item in `main_data.txt` must be classified into one of 7 classes, using the training set `train_data.txt`.\n",
    "* Each line of `train_data.txt` defines 1 training item, as a vector of floating-point values, followed by its class label (a single integer in the range 1-7).\n",
    "* Each line of `main_data.txt` defines 1 data item, as only a vector of floating-point values, without a class label.\n",
    "\n",
    "To do this write 3 functions:\n",
    "\n",
    "- **`data_setup()`** which reads in both the dataset contained in `main_data.txt`, and the training dataset contained in `train_data.txt` into two lists - returning the two lists (`main_data`, and `train_data`) from the function. The lists containing each item of the dataset should use appropriate types for each element of the list. \n",
    "- **`dist_vect(item,train_data)`** which takes as arguments one element of the list `main_data`, and the list `train_data`, both from the output of the data_setup function. The function should calculate the Euclidean distance from item in the dataset to each item in the training dataset in order. The function should return a list of (distance, class) tuples.\n",
    "- **`decide_class(dv)`** which takes the output list of tuples of the dist_vect function as an argument, and uses it to return the class of the item in the dataset.\n",
    "\n",
    "- Lastly use your functions to obtain a list containing the calculated class for each of the element in the `main_data` list you have constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 7, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 3, 2, 7, 1, 1, 1, 1, 2, 6, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statistics as st\n",
    "\n",
    "def data_setup():\n",
    "    main_data=[]\n",
    "    train_data=[]\n",
    "    with open('main_data.txt') as m: #open file \n",
    "        for line in m:\n",
    "            m_inner_list = [float(elt.strip()) for elt in line.split(',')] #convert each line to inner list \n",
    "            main_data.append(m_inner_list) #find list of lists\n",
    "    with open('train_data.txt') as t:\n",
    "        for line in t:\n",
    "            t_inner_list = [float(elt.strip()) for elt in line.split(',')]\n",
    "            train_data.append(t_inner_list)\n",
    "    \n",
    "    return main_data,train_data\n",
    "\n",
    "\n",
    "def dist_vect(item,train_data):\n",
    "    a=[]\n",
    "    for t_item in train_data: #loop over train_data\n",
    "        distance=0\n",
    "        for j in range(len(item)): #sum distance in order\n",
    "            distance=distance+np.sqrt((item[j]-t_item[j])**2)\n",
    "            t=distance,t_item[7]\n",
    "            a.append(t) #create list of tuples\n",
    "    e=sorted(a,key=lambda tup:tup[0]) #sorted by distance\n",
    "    dv=[e[0],e[1],e[2]] #find the 3 nearest neighbours\n",
    "    return dv  \n",
    "\n",
    "\n",
    "def decide_class(dv):\n",
    "    votes=[x[1] for x in dv] #list of class\n",
    "    if votes[0]!=votes[1]!=votes[2]: \n",
    "        item_class=int(votes[0]) #chose the nearest one\n",
    "    else:\n",
    "        item_class=int(st.mode(votes)) #find the most frequent class\n",
    "               \n",
    "    return item_class\n",
    "\n",
    "cal_class=[]\n",
    "for item in data_setup()[0]: #loop over main_data\n",
    "    dv=dist_vect(item,data_setup()[1]) #mplement all three functions to find the class of main_data\n",
    "    cal_class.append(decide_class(dv)) \n",
    "print(cal_class)   #print out the final result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2b - Changing the distance measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next write two different replacements for the **dist_vect(item,train_data)** function. The function you have written above calculates the distance between data items using Euclidean distance as:\n",
    "$$d(x,y) = \\left(\\sum_{i=1}^n (x_i-y_i)^2\\right)^{1/2}$$\n",
    "\n",
    "For this task write two more versions of the distance function using different distance metrics.\n",
    "\n",
    "- Firstly **dist_vectL1(item,train_data)** where distance is determined by using:\n",
    "\n",
    "$$d(x,y) = \\sum_{i=1}^n \\left|x_i-y_i\\right|$$\n",
    "\n",
    "- Next, **dist_vectLinf(item,train_data)** where distance is determined by using:\n",
    "\n",
    "$$d(x,y) = \\max\\limits_{i=1..n}\\left|x_i-y_i\\right|$$\n",
    "\n",
    "These are the $L_1$ and $L_\\infty$ norms respectively. \n",
    "\n",
    "- Lastly use the new functions to obtain a list containing the calculated class for each of the element in the main_data list you have constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.999999999990898e-05, 1.0), (0.000180000000000069, 5.0), (0.00019999999999997797, 2.0)]\n",
      "1\n",
      "[1, 1, 7, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 3, 2, 7, 1, 1, 1, 1, 2, 6, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "import statistics as st\n",
    "\n",
    "def data_setup():\n",
    "    main_data=[]\n",
    "    train_data=[]\n",
    "    with open('main_data.txt') as m: #open file \n",
    "        for line in m:\n",
    "            m_inner_list = [float(elt.strip()) for elt in line.split(',')] #convert each line to inner list \n",
    "            main_data.append(m_inner_list) #find list of lists\n",
    "    with open('train_data.txt') as t:\n",
    "        for line in t:\n",
    "            t_inner_list = [float(elt.strip()) for elt in line.split(',')]\n",
    "            train_data.append(t_inner_list)\n",
    "    \n",
    "    return main_data,train_data\n",
    "\n",
    "\n",
    "\n",
    "def dist_vectL1(item,train_data):\n",
    "    a=[]\n",
    "    for t_item in train_data: #loop over train_data\n",
    "        distance=0\n",
    "        for j in range(len(item)): #sum distance in order\n",
    "            distance=distance+abs(item[j]-t_item[j])\n",
    "            t=distance,t_item[7]\n",
    "            a.append(t) #create list of tuples\n",
    "    e=sorted(a,key=lambda tup:tup[0]) #sorted by distance\n",
    "    dv=[e[0],e[1],e[2]] #find the 3 nearest neighbours\n",
    "    return dv  \n",
    "\n",
    "\n",
    "def decide_class(dv):\n",
    "    votes=[x[1] for x in dv] #list of class\n",
    "    if votes[0]!=votes[1]!=votes[2]: \n",
    "        item_class=int(votes[0]) #chose the nearest one\n",
    "    else:\n",
    "        item_class=int(st.mode(votes)) #find the most frequent class\n",
    "               \n",
    "    return item_class\n",
    "\n",
    "cal_class=[]\n",
    "for item in data_setup()[0]: #loop over main_data\n",
    "    dv=dist_vectL1(item,data_setup()[1])\n",
    "    cal_class.append(decide_class(dv)) \n",
    "    \n",
    "print(dist_vect(data_setup()[0][0],data_setup()[1]))\n",
    "print(decide_class(dist_vectL1(data_setup()[0][0],data_setup()[1])))\n",
    "print(cal_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 5, 2, 6, 6, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def data_setup():\n",
    "    main_data=[]\n",
    "    train_data=[]\n",
    "    with open('main_data.txt') as m: #open file \n",
    "        for line in m:\n",
    "            m_inner_list = [float(elt.strip()) for elt in line.split(',')] #convert each line to inner list \n",
    "            main_data.append(m_inner_list) #find list of lists\n",
    "    with open('train_data.txt') as t:\n",
    "        for line in t:\n",
    "            t_inner_list = [float(elt.strip()) for elt in line.split(',')]\n",
    "            train_data.append(t_inner_list)\n",
    "    \n",
    "    return main_data,train_data\n",
    "\n",
    "\n",
    "def dist_vectLinf(item,train_data):\n",
    "    a=[]\n",
    "    f=[]\n",
    "    for t_item in train_data: #loop over train_data\n",
    "        t_item_n=np.delete(t_item,7,axis=0)\n",
    "        distance=np.linalg.norm(np.degrees(item)-np.degrees(t_item_n),ord=np.inf)    \n",
    "        t=distance,t_item[7]\n",
    "        a.append(t) #create list of tuples\n",
    "    e=sorted(a,key=lambda tup:tup[0]) #sorted by distance\n",
    "    dv=[e[0],e[1],e[2]] #find the 3 nearest neighbours\n",
    "    return dv  \n",
    "\n",
    "\n",
    "def decide_class(dv):\n",
    "    votes=[x[1] for x in dv] #list of class\n",
    "    if votes[0]!=votes[1]!=votes[2]: \n",
    "        item_class=votes[0] #chose the nearest one\n",
    "    else:\n",
    "        item_class=st.mode(votes) #find the most frequent class\n",
    "    return int(item_class)\n",
    "\n",
    "cal_class=[]\n",
    "for item in data_setup()[0]: #loop over main_data\n",
    "    dv=dist_vectLinf(item,data_setup()[1])\n",
    "    cal_class.append(decide_class(dv)) \n",
    "\n",
    "print(cal_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2c - Changing the decision methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple majority-voting implemented in your `decide_class()` function is unweighted --- once the 3 nearest neighbours are found, they each have 1 vote, regardless of which is closest (unless there is a tie). An alternative is *weighted voting*, where each of the $k$ nearest neighbours is attributed a coefficient, such that the closest neighbours carry more weight in the result.\n",
    "\n",
    "Write a function `decide_class_wk(dv,k)` which also takes the number $k$ of nearest neighbours as an input argument. Your function should attribute a weight $w_j$ to each of the $k$ nearest neighbours, inversely proportional to their distance from the data item:\n",
    "\n",
    "$$\n",
    "w_j = \\frac{1}{d_j(x,y)}\\, , \\quad j \\in \\{1,\\ldots,k\\}\n",
    "$$\n",
    "\n",
    "The score for each class is determined by the sum of the weights of each item in that class amongst the $k$ nearest neighbours. The class with the highest score is chosen for the data item.\n",
    "\n",
    "(Note that the simple majority-voting corresponds to setting all the weights to 1.)\n",
    "\n",
    "Test your function on the dataset, for $k=3, 8,$ and $25$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:35: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 3.0, 2.0, 7.0, 1.0, 1.0, 1.0, 3.0, 2.0, 6.0, 2.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def data_setup():\n",
    "    main_data=[]\n",
    "    train_data=[]\n",
    "    with open('main_data.txt') as m: #open file \n",
    "        for line in m:\n",
    "            m_inner_list = [float(elt.strip()) for elt in line.split(',')] #convert each line to inner list \n",
    "            main_data.append(m_inner_list) #find list of lists\n",
    "    with open('train_data.txt') as t:\n",
    "        for line in t:\n",
    "            t_inner_list = [float(elt.strip()) for elt in line.split(',')]\n",
    "            train_data.append(t_inner_list)\n",
    "    \n",
    "    return main_data,train_data\n",
    "\n",
    "\n",
    "def dist_vect(item,train_data):\n",
    "    a=[]\n",
    "    for t_item in train_data: #loop over train_data\n",
    "        distance=0\n",
    "        for j in range(len(item)): #sum distance in order\n",
    "            distance=distance+np.sqrt((item[j]-t_item[j])**2)\n",
    "            t=distance,t_item[7]\n",
    "    a.append(t) #create list of tuples\n",
    "    dv=sorted(a,key=lambda tup:tup[0])\n",
    "    return dv  \n",
    "\n",
    "\n",
    "def decide_class(dv,k):\n",
    "    c=[]\n",
    "    d=[]\n",
    "    for v in range(k):\n",
    "        w=1/dv[v][0]\n",
    "        c.append(w)\n",
    "        d.append(dv[v][1])\n",
    "        ndv=zip(c,d) #create a new tuple with weight and class\n",
    "    a={}\n",
    "    b=[]\n",
    "    for i,j in ndv:\n",
    "        a.setdefault(j,[]).append(i) # convert the tuple into a dictionary, which looks like{1:[1.2,3.2],2:[1,2]...}\n",
    "    item_class=max(a, key=lambda k: sum(a[k])) # find class correponding to maximum weights by summing all values of a single key\n",
    "    return int(item_class)\n",
    "\n",
    "cal_class=[]\n",
    "k=3\n",
    "for item in data_setup()[0]: #loop over main_data\n",
    "    dv=dist_vect(item,data_setup()[1])\n",
    "    cal_class.append(decide_class(dv,k)) \n",
    "print(cal_class)             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
